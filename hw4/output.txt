### PART 1 ###

Length: 752

Top 5 most occurring rules with counts: 
PUNC -> . # 346
TO -> to # 241
PP -> IN NP_NNP # 239
IN -> from # 218
PP -> IN NP # 197

Top 5 most occurring rules with percents: 
NP_NP_EX -> there # 1.0
FRAG_NP_NN -> <unk> # 1.0
X_JJ -> nonstop # 1.0
ADVP_RBS -> latest # 1.0
WRB -> How # 1.0

### PART 2 ###

FIRST FIVE LINES WITH LOG PROBABILITIES
(TOP (S (NP (DT The) (NN flight)) (VP (MD should) (VP (VB be) (NP (NP* (CD eleven) (RB a.m)) (NN tomorrow))))) (PUNC .))
probability: -40.86005862039388

(TOP (S (S* (S (NP_PRP I) (VP (MD would) (VP (VP* (VP* (VB like) (NP_PRP it)) (X_TO to)) (VP (VBP have) (NP (NP (DT a) (NN stop)) (PP (IN in) (NP (NNP New) (NNP York)))))))) (CC and)) (S (NP_PRP I) (VP (MD would) (VP (VB like) (NP (NP (DT a) (NN flight)) (SBAR (WHNP_WDT that) (S_VP (VBZ serves) (ADVP_RB <unk>)))))))) (PUNC .))
probability: -84.96026208986648

(TOP (SBARQ (WHNP (WHNP_WDT Which) (PP (IN of) (NP_DT these))) (SQ_VP (VBP serve) (NP_NN dinner))) (PUNC ?))
probability: -20.205461711126294

(TOP (SBARQ (WHNP (WDT Which) (NNS ones)) (SQ_VP (VBP stop) (PP (IN in) (NP_NNP Nashville)))) (PUNC ?))
probability: -23.54035303502083

(TOP (SQ (VBP <unk>) (SQ* (NP_NP_EX there) (SQ* (NP (DT any) (NNS flights)) (VP (VBG arriving) (PP (IN after) (NP (CD eleven) (RB a.m))))))) (PUNC ?))
probability: -37.278934919588785

python evalb.py dev.parses.post dev.trees                          
dev.parses.post 435 brackets
dev.trees       474 brackets
matching        400 brackets
precision       0.919540229885
recall  0.84388185654
F1      0.880088008801


### PART 3 ###

Note: These changes are noted in the code by being surrounded by large amounts of ##### characters.

Modification 1: Add Delta smoothing: I added very light add delta smoothing.
new dev f1: 0.905450500556

Modification 2: Vertical Markovization: For each rule, I added the parent’s label to that rules base. If this could not find a parse, I resorted back to the best rule that did not take vertical markovization into account if one existed.
new dev f1: 0.90848952591

Modification 3: I wanted to increase the probability that <unk> would occur in the best tree (as test would theoretically have more unk words). For each base, I added 30 occurrences of that rule going to “<unk>” during my training.
new dev f1: 0.878048780488

Using all three modifications:

dev.parses.post 425 brackets
dev.trees       474 brackets
matching        407 brackets
precision       0.957647058824
recall  0.85864978903
F1      0.905450500556

test.parses.post        425 brackets
test.trees      471 brackets
matching        406 brackets
precision       0.955294117647
recall  0.861995753715
F1      0.90625


What helped and what didn’t?:
All three of these modifications helped. The Vertical Markovization helped the most.  The increased unk probability helped a lot as well when paired with vertical markovization, but actually hurt the f1 score by itself. The add delta smoothing helped a bit, but in a very small way when combined with the other two. By itself, add delta smoothing helped a lot.

